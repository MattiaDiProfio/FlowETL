[2025-05-31T15:05:08.585+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-31T15:06:03.036+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: PlanningEngine.publishOptimalPlan manual__2025-05-31T15:04:13.949925+00:00 [queued]>
[2025-05-31T15:06:03.043+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: PlanningEngine.publishOptimalPlan manual__2025-05-31T15:04:13.949925+00:00 [queued]>
[2025-05-31T15:06:03.044+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2025-05-31T15:05:08.206+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): publishOptimalPlan> on 2025-05-31 15:04:13.949925+00:00
[2025-05-31T15:05:08.213+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=113) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-05-31T15:05:08.215+0000] {standard_task_runner.py:72} INFO - Started process 114 to run task
[2025-05-31T15:05:08.215+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'PlanningEngine', 'publishOptimalPlan', 'manual__2025-05-31T15:04:13.949925+00:00', '--job-id', '3451', '--raw', '--subdir', 'DAGS_FOLDER/***DAG.py', '--cfg-path', '/tmp/tmpk0efq6kr']
[2025-05-31T15:05:08.216+0000] {standard_task_runner.py:105} INFO - Job 3451: Subtask publishOptimalPlan
[2025-05-31T15:05:08.266+0000] {task_command.py:467} INFO - Running <TaskInstance: PlanningEngine.publishOptimalPlan manual__2025-05-31T15:04:13.949925+00:00 [running]> on host 34f0c172c2da
[2025-05-31T15:05:08.351+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='PlanningEngine' AIRFLOW_CTX_TASK_ID='publishOptimalPlan' AIRFLOW_CTX_EXECUTION_DATE='2025-05-31T15:04:13.949925+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-31T15:04:13.949925+00:00'
[2025-05-31T15:05:08.352+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-05-31T15:05:08.378+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9091 <connecting> [IPv4 ('172.18.0.5', 9091)]>: connecting to kafka:9091 [('172.18.0.5', 9091) IPv4]
[2025-05-31T15:05:08.379+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2025-05-31T15:05:08.380+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9091 <connecting> [IPv4 ('172.18.0.5', 9091)]>: Connection complete.
[2025-05-31T15:05:08.484+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2025-05-31T15:05:08.485+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2025-05-31T15:05:08.487+0000] {airflowDAG.py:411} INFO - Connected to broker
[2025-05-31T15:05:08.490+0000] {airflowDAG.py:415} INFO - Optimal plan {'schema': {'CHEMISTRY_ID': 'string', 'CHEMISTRY_TYPE': 'string', 'ASSAY_CATEGORY': 'string', 'ASSAY_NAME': 'string', 'ACTIVITY_WITH_UNITS': 'string', 'ASSAY_QUAL_RESULT': 'boolean', 'CONCENTRATION_WITH_UNITS': 'string', 'ASSAY_DATE': 'string', 'DEPARTMENT_ID': 'boolean', 'BIOMASS_ID': 'string', 'ORGANISM_ID': 'string', 'SITE_ID': 'string', 'REGION': 'string', 'LATITUDE_LONGITUDE': 'string', 'PRODUCTION_MEDIA': 'string', 'SOURCE_ID': 'string', 'SOURCE_TYPE': 'string', 'PERSON_ID': 'string', 'COUNTRY': 'string', 'PHYLUM_FAMILY_GENUS_SPECIES': 'string', 'COMMON_NAME': 'boolean', 'FAMILY': 'string', 'GENUS': 'string', 'SPECIES': 'boolean'}, 'plan': ['chemistry_source.csv', {'associated_key': None}, {'standardiseFeatures': '```python\n{\n    ("REGION",): ("REGION",),\n    ("COUNTRY",): ("COUNTRY",),\n    ("LATITUDE", "LONGITUDE"): ("LATITUDE_LONGITUDE",),\n    ("SITE_ID",): ("SITE_ID",),\n    ("PERSON_ID",): ("PERSON_ID",),\n    ("SOURCE_TYPE",): ("SOURCE_TYPE",),\n    ("SOURCE_ID",): ("SOURCE_ID",),\n    ("PHYLUM", "FAMILY", "GENUS", "SPECIES"): ("PHYLUM_FAMILY_GENUS_SPECIES",),\n    ("ORGANISM_ID",): ("ORGANISM_ID",),\n    ("PRODUCTION_MEDIA",): ("PRODUCTION_MEDIA",),\n    ("BIOMASS_ID",): ("BIOMASS_ID",),\n    ("CHEMISTRY_TYPE",): ("CHEMISTRY_TYPE",),\n    ("CHEMISTRY_ID",): ("CHEMISTRY_ID",),\n    ("ASSAY_CATEGORY",): ("ASSAY_CATEGORY",),\n    ("ASSAY_NAME",): ("ASSAY_NAME",),\n    ("CONCENTRATION", "CONCENTRATION_UNITS"): ("CONCENTRATION_WITH_UNITS",),\n    ("ACTIVITY", "UNITS"): ("ACTIVITY_WITH_UNITS",),\n    ("ASSAY_QUAL_RESULT",): ("ASSAY_QUAL_RESULT",),\n    ("ASSAY_DATE",): ("ASSAY_DATE",),\n    ("DEPARTMENT_ID",): ("DEPARTMENT_ID",)\n}\n```'}, 'missingValues/impute', 'duplicates', 'outliers/impute', {'standardiseValues': '```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data = input_table[1:]\n    \n    # Creating output table with correct headers\n    output_table = [\n        ["REGION", "COUNTRY", "LATITUDE_LONGITUDE", "SITE_ID", "PERSON_ID", \n         "SOURCE_TYPE", "SOURCE_ID", "PHYLUM_FAMILY_GENUS_SPECIES", "ORGANISM_ID", \n         "PRODUCTION_MEDIA", "BIOMASS_ID", "CHEMISTRY_TYPE", "CHEMISTRY_ID", \n         "ASSAY_CATEGORY", "ASSAY_NAME", "CONCENTRATION_WITH_UNITS", \n         "ACTIVITY_WITH_UNITS", "ASSAY_QUAL_RESULT", "ASSAY_DATE", "DEPARTMENT_ID"]\n    ]\n    \n    for row in data:\n        # Create a dictionary for easier column access\n        row_dict = {headers[i]: row[i] for i in range(len(headers))}\n        \n        # Create the combined phylum_family_genus_species field\n        phylum = row_dict.get("PHYLUM_FAMILY_GENUS_SPECIES", "")\n        family = row_dict.get("FAMILY", "")\n        genus = row_dict.get("GENUS", "")\n        species = row_dict.get("SPECIES", "")\n        \n        # Combine with pipe separator, excluding empty values\n        phylum_family_genus_species_parts = [phylum, family, genus, species]\n        phylum_family_genus_species = "|".join([part for part in phylum_family_genus_species_parts if part])\n        \n        # Handle special _ext_ values\n        for key, value in row_dict.items():\n            if value == "_ext_":\n                row_dict[key] = "_ext_"\n                \n        transformed_row = [\n            row_dict.get("REGION", ""),\n            row_dict.get("COUNTRY", ""),\n            row_dict.get("LATITUDE_LONGITUDE", ""),\n            row_dict.get("SITE_ID", ""),\n            row_dict.get("PERSON_ID", ""),\n            row_dict.get("SOURCE_TYPE", ""),\n            row_dict.get("SOURCE_ID", ""),\n            phylum_family_genus_species,\n            row_dict.get("ORGANISM_ID", ""),\n            row_dict.get("PRODUCTION_MEDIA", ""),\n            row_dict.get("BIOMASS_ID", ""),\n            row_dict.get("CHEMISTRY_TYPE", ""),\n            row_dict.get("CHEMISTRY_ID", ""),\n            row_dict.get("ASSAY_CATEGORY", ""),\n            row_dict.get("ASSAY_NAME", ""),\n            row_dict.get("CONCENTRATION_WITH_UNITS", ""),\n            row_dict.get("ACTIVITY_WITH_UNITS", ""),\n            row_dict.get("ASSAY_QUAL_RESULT", ""),\n            row_dict.get("ASSAY_DATE", ""),\n            row_dict.get("DEPARTMENT_ID", "")\n        ]\n        \n        output_table.append(transformed_row)\n        \n    return output_table\n```'}]} published to broker
[2025-05-31T15:05:08.490+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2025-05-31T15:05:08.491+0000] {conn.py:362} INFO - <BrokerConnection node_id=1001 host=kafka:9091 <connecting> [IPv4 ('172.18.0.5', 9091)]>: connecting to kafka:9091 [('172.18.0.5', 9091) IPv4]
[2025-05-31T15:05:08.492+0000] {conn.py:393} INFO - <BrokerConnection node_id=1001 host=kafka:9091 <connecting> [IPv4 ('172.18.0.5', 9091)]>: Connection complete.
[2025-05-31T15:05:08.492+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9091 <connected> [IPv4 ('172.18.0.5', 9091)]>: Closing connection. 
[2025-05-31T15:05:08.497+0000] {conn.py:673} INFO - <BrokerConnection node_id=1001 host=kafka:9091 <connected> [IPv4 ('172.18.0.5', 9091)]>: Closing connection. 
[2025-05-31T15:05:08.498+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-05-31T15:05:08.508+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-05-31T15:05:08.509+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=PlanningEngine, task_id=publishOptimalPlan, run_id=manual__2025-05-31T15:04:13.949925+00:00, execution_date=20250531T150413, start_date=20250531T150603, end_date=20250531T150508
[2025-05-31T15:05:08.559+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-05-31T15:05:08.579+0000] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-05-31T15:05:08.582+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
