root INFO - 01/05/2025 04:17:55 PM : Connected to broker
kafka.consumer.subscription_state INFO - 01/05/2025 04:17:55 PM : Updated partition assignment: [TopicPartition(topic='optimalPlans', partition=0)]
kafka.conn INFO - 01/05/2025 04:17:55 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 01/05/2025 04:17:55 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 01/05/2025 04:17:55 PM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 01/05/2025 04:19:12 PM : Received optimal plan -> ['amazon_stock_data_source.csv', {'associated_key': None}, {'standardiseFeatures': '```python\n{\n    ("date",): ("date",),\n    ("open",): ("open_price",),\n    ("high",): ("daily_high",),\n    ("low",): ("daily_low",),\n    ("close",): ("close_price",),\n    ("volume",): ("trade_volume_millions",),\n    ("adj_close",): ()\n}\n```'}, 'missingValues/impute', 'duplicates', 'outliers/impute', {'standardiseValues': '```python\ndef transform_table(input_table):\n    if len(input_table) <= 1:\n        return input_table\n    \n    # Note: The column mapping seems to suggest a different set of columns than what\'s in the input.\n    # Working with the actual input column names.\n    \n    # Get header\n    header = input_table[0]\n    data_rows = input_table[1:]\n    \n    result = [header]\n    \n    for row in data_rows:\n        new_row = []\n        \n        # Process each column\n        for i, value in enumerate(row):\n            column_name = header[i]\n            \n            # Skip processing \'_ext_\' values\n            if value == \'_ext_\':\n                new_row.append(value)\n                continue\n                \n            # Process date column - convert to "YYYY/MM/DD" format\n            if column_name == \'date\' and value:\n                try:\n                    # Extract date part (first 10 chars) and reformat\n                    date_part = value.split(\' \')[0]\n                    components = date_part.split(\'-\')\n                    if len(components) == 3:\n                        new_row.append(f"{components[0]}/{components[1]}/{components[2]}")\n                    else:\n                        new_row.append(value)\n                except:\n                    new_row.append(value)\n            # Convert trade volume from absolute to millions\n            elif column_name == \'trade_volume_millions\' and value:\n                try:\n                    volume = float(value)\n                    # If volume is already in millions, keep as is\n                    # If it\'s in absolute units, convert to millions\n                    if volume > 1000000:\n                        new_row.append(f"{volume / 1000000:.3f}")\n                    else:\n                        new_row.append(f"{volume:.3f}")\n                except:\n                    new_row.append(value)\n            # Format numeric values with 3 decimal places\n            elif column_name in [\'open_price\', \'daily_high\', \'daily_low\', \'close_price\'] and value:\n                try:\n                    new_row.append(f"{float(value):.3f}")\n                except:\n                    new_row.append(value)\n            else:\n                new_row.append(value)\n                \n        result.append(new_row)\n    \n    return result\n```'}]
root INFO - 01/05/2025 04:19:12 PM : Published the following metrics to Reporter : {'from': 'pre_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 40.182}, 'outliers': {'numerical_outliers_percent': 2.287}, 'duplicates': {'duplicate_rows_percent': 16.451}, 'dq': 0.804}}
kafka.conn INFO - 01/05/2025 04:19:12 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 01/05/2025 04:19:12 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 01/05/2025 04:19:12 PM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 01/05/2025 04:19:12 PM : Applied plan to file 'input\source\amazon_stock_data_source.csv'
root INFO - 01/05/2025 04:19:12 PM : Successfully loaded the transformed file to '\output'
root INFO - 01/05/2025 04:19:12 PM : Published the following metrics to Reporter : {'from': 'post_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 0.0}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 12.526}, 'dq': 0.958}}
kafka.producer.kafka INFO - 01/05/2025 04:19:12 PM : Closing the Kafka producer with 4294967.0 secs timeout.
kafka.conn INFO - 01/05/2025 04:19:12 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
kafka.conn INFO - 01/05/2025 04:19:12 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
