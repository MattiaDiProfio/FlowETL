root INFO - 31/05/2025 04:04:55 PM : Connected to broker
kafka.consumer.subscription_state INFO - 31/05/2025 04:04:55 PM : Updated partition assignment: [TopicPartition(topic='optimalPlans', partition=0)]
kafka.conn INFO - 31/05/2025 04:04:55 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 31/05/2025 04:04:55 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 31/05/2025 04:04:55 PM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 31/05/2025 04:06:03 PM : Received optimal plan -> ['chemistry_source.csv', {'associated_key': None}, {'standardiseFeatures': '```python\n{\n    ("REGION",): ("REGION",),\n    ("COUNTRY",): ("COUNTRY",),\n    ("LATITUDE", "LONGITUDE"): ("LATITUDE_LONGITUDE",),\n    ("SITE_ID",): ("SITE_ID",),\n    ("PERSON_ID",): ("PERSON_ID",),\n    ("SOURCE_TYPE",): ("SOURCE_TYPE",),\n    ("SOURCE_ID",): ("SOURCE_ID",),\n    ("PHYLUM", "FAMILY", "GENUS", "SPECIES"): ("PHYLUM_FAMILY_GENUS_SPECIES",),\n    ("ORGANISM_ID",): ("ORGANISM_ID",),\n    ("PRODUCTION_MEDIA",): ("PRODUCTION_MEDIA",),\n    ("BIOMASS_ID",): ("BIOMASS_ID",),\n    ("CHEMISTRY_TYPE",): ("CHEMISTRY_TYPE",),\n    ("CHEMISTRY_ID",): ("CHEMISTRY_ID",),\n    ("ASSAY_CATEGORY",): ("ASSAY_CATEGORY",),\n    ("ASSAY_NAME",): ("ASSAY_NAME",),\n    ("CONCENTRATION", "CONCENTRATION_UNITS"): ("CONCENTRATION_WITH_UNITS",),\n    ("ACTIVITY", "UNITS"): ("ACTIVITY_WITH_UNITS",),\n    ("ASSAY_QUAL_RESULT",): ("ASSAY_QUAL_RESULT",),\n    ("ASSAY_DATE",): ("ASSAY_DATE",),\n    ("DEPARTMENT_ID",): ("DEPARTMENT_ID",)\n}\n```'}, 'missingValues/impute', 'duplicates', 'outliers/impute', {'standardiseValues': '```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data = input_table[1:]\n    \n    # Creating output table with correct headers\n    output_table = [\n        ["REGION", "COUNTRY", "LATITUDE_LONGITUDE", "SITE_ID", "PERSON_ID", \n         "SOURCE_TYPE", "SOURCE_ID", "PHYLUM_FAMILY_GENUS_SPECIES", "ORGANISM_ID", \n         "PRODUCTION_MEDIA", "BIOMASS_ID", "CHEMISTRY_TYPE", "CHEMISTRY_ID", \n         "ASSAY_CATEGORY", "ASSAY_NAME", "CONCENTRATION_WITH_UNITS", \n         "ACTIVITY_WITH_UNITS", "ASSAY_QUAL_RESULT", "ASSAY_DATE", "DEPARTMENT_ID"]\n    ]\n    \n    for row in data:\n        # Create a dictionary for easier column access\n        row_dict = {headers[i]: row[i] for i in range(len(headers))}\n        \n        # Create the combined phylum_family_genus_species field\n        phylum = row_dict.get("PHYLUM_FAMILY_GENUS_SPECIES", "")\n        family = row_dict.get("FAMILY", "")\n        genus = row_dict.get("GENUS", "")\n        species = row_dict.get("SPECIES", "")\n        \n        # Combine with pipe separator, excluding empty values\n        phylum_family_genus_species_parts = [phylum, family, genus, species]\n        phylum_family_genus_species = "|".join([part for part in phylum_family_genus_species_parts if part])\n        \n        # Handle special _ext_ values\n        for key, value in row_dict.items():\n            if value == "_ext_":\n                row_dict[key] = "_ext_"\n                \n        transformed_row = [\n            row_dict.get("REGION", ""),\n            row_dict.get("COUNTRY", ""),\n            row_dict.get("LATITUDE_LONGITUDE", ""),\n            row_dict.get("SITE_ID", ""),\n            row_dict.get("PERSON_ID", ""),\n            row_dict.get("SOURCE_TYPE", ""),\n            row_dict.get("SOURCE_ID", ""),\n            phylum_family_genus_species,\n            row_dict.get("ORGANISM_ID", ""),\n            row_dict.get("PRODUCTION_MEDIA", ""),\n            row_dict.get("BIOMASS_ID", ""),\n            row_dict.get("CHEMISTRY_TYPE", ""),\n            row_dict.get("CHEMISTRY_ID", ""),\n            row_dict.get("ASSAY_CATEGORY", ""),\n            row_dict.get("ASSAY_NAME", ""),\n            row_dict.get("CONCENTRATION_WITH_UNITS", ""),\n            row_dict.get("ACTIVITY_WITH_UNITS", ""),\n            row_dict.get("ASSAY_QUAL_RESULT", ""),\n            row_dict.get("ASSAY_DATE", ""),\n            row_dict.get("DEPARTMENT_ID", "")\n        ]\n        \n        output_table.append(transformed_row)\n        \n    return output_table\n```'}]
root INFO - 31/05/2025 04:06:06 PM : Published the following metrics to Reporter : {'from': 'pre_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 10.717}, 'outliers': {'numerical_outliers_percent': 2.483}, 'duplicates': {'duplicate_rows_percent': 3.127}, 'dq': 0.946}}
kafka.conn INFO - 31/05/2025 04:06:06 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 31/05/2025 04:06:06 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 31/05/2025 04:06:06 PM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 31/05/2025 04:06:09 PM : Applied plan to file 'input\source\chemistry_source.csv'
root INFO - 31/05/2025 04:06:10 PM : Successfully loaded the transformed file to '\output'
root INFO - 31/05/2025 04:06:10 PM : Published the following metrics to Reporter : {'from': 'post_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 0.0}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 0.002}, 'dq': 1.0}}
kafka.producer.kafka INFO - 31/05/2025 04:06:10 PM : Closing the Kafka producer with 4294967.0 secs timeout.
kafka.conn INFO - 31/05/2025 04:06:10 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
kafka.conn INFO - 31/05/2025 04:06:10 PM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
