root INFO - 14/04/2025 11:54:57 AM : Connected to broker
kafka.consumer.subscription_state INFO - 14/04/2025 11:54:57 AM : Updated partition assignment: [TopicPartition(topic='optimalPlans', partition=0)]
kafka.conn INFO - 14/04/2025 11:54:57 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 14/04/2025 11:54:57 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 14/04/2025 11:54:57 AM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 14/04/2025 11:55:31 AM : Received optimal plan -> ['news_categories_source.json', {'associated_key': None}, {'standardiseFeatures': '```python\n{\n    ("link",): ("article_link",),\n    ("headline",): ("headline",),\n    ("authors",): ("authors",),\n    ("category",): ("category",),\n    ("date",): ("published_date",),\n    ("short_description",): ()\n}\n```'}, 'missingValues/impute', 'duplicates', 'outliers/impute', {'standardiseValues': '```python\ndef transform_table(input_table):\n    if not input_table or len(input_table) <= 1:\n        return input_table\n    \n    headers = input_table[0]\n    data = input_table[1:]\n    output_table = []\n    \n    # Define the new column order\n    new_headers = ["article_link", "headline", "authors", "category", "published_date"]\n    output_table.append(new_headers)\n    \n    for row in data:\n        if len(row) != len(headers):\n            continue\n            \n        row_dict = {headers[i]: row[i] for i in range(len(headers))}\n        \n        # Format the date\n        date_parts = row_dict[\'published_date\'].split(\'-\')\n        if len(date_parts) == 3:\n            year, month, day = date_parts\n            months = [\'January\', \'February\', \'March\', \'April\', \'May\', \'June\', \n                      \'July\', \'August\', \'September\', \'October\', \'November\', \'December\']\n            month_num = int(month)\n            if 1 <= month_num <= 12:\n                formatted_date = f"{day}-{months[month_num-1]}-{year}"\n            else:\n                formatted_date = row_dict[\'published_date\']\n        else:\n            formatted_date = row_dict[\'published_date\']\n        \n        # Format the article link to use huffingtonpost.com instead of huffpost.com\n        article_link = row_dict[\'article_link\'].replace(\'huffpost.com\', \'huffingtonpost.com\')\n        \n        # Format the authors field\n        authors = row_dict[\'authors\']\n        if authors and ", AP" in authors:\n            authors = authors.replace(", AP", ", Contributor\\nAssociated Press")\n        elif authors:\n            parts = authors.split(\', \')\n            if len(parts) == 2:\n                authors = f"{authors}, Contributor"\n        \n        # Format the category to be lowercase\n        category = row_dict[\'category\'].lower()\n        \n        # Create the output row\n        output_row = [article_link, row_dict[\'headline\'], authors, category, formatted_date]\n        \n        # Check if any cell has \'_ext_\' and keep it unchanged\n        for i, val in enumerate(output_row):\n            if val == \'_ext_\':\n                output_row[i] = \'_ext_\'\n                \n        output_table.append(output_row)\n        \n    return output_table\n```'}]
root INFO - 14/04/2025 11:55:31 AM : Published the following metrics to Reporter : {'from': 'pre_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 2.063}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 0.476}, 'dq': 0.995}}
kafka.conn INFO - 14/04/2025 11:55:31 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 14/04/2025 11:55:31 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 14/04/2025 11:55:31 AM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 14/04/2025 11:55:31 AM : Applied plan to file 'input\source\news_categories_source.json'
root INFO - 14/04/2025 11:55:31 AM : Successfully loaded the transformed file to '\output'
root INFO - 14/04/2025 11:55:31 AM : Published the following metrics to Reporter : {'from': 'post_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 0.0}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 0.476}, 'dq': 0.998}}
kafka.producer.kafka INFO - 14/04/2025 11:55:31 AM : Closing the Kafka producer with 4294967.0 secs timeout.
kafka.conn INFO - 14/04/2025 11:55:31 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
kafka.conn INFO - 14/04/2025 11:55:31 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
