root INFO - 07/04/2025 11:07:44 AM : Connected to broker
kafka.consumer.subscription_state INFO - 07/04/2025 11:07:44 AM : Updated partition assignment: [TopicPartition(topic='optimalPlans', partition=0)]
kafka.conn INFO - 07/04/2025 11:07:44 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 07/04/2025 11:07:44 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 07/04/2025 11:07:44 AM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 07/04/2025 11:08:46 AM : Received optimal plan -> ['recipes_source.json', {'associated_key': None}, {'standardiseFeatures': '```python\n{\n    ("id",): ("recipe_id",),\n    ("ingredients",): ("ingredients_list",),\n    ("cuisine",): ("cuisine",)\n}\n```'}, 'missingValues/impute', 'duplicates', 'outliers/impute', {'standardiseValues': '```python\ndef transform_table(input_table):\n    header = input_table[0]\n    data_rows = input_table[1:]\n    \n    # Initialize the output table with the header\n    output_table = [["recipe_id", "cuisine", "ingredients_list"]]\n    \n    for row in data_rows:\n        new_row = []\n        \n        # Get values from input row based on column positions\n        recipe_id = row[header.index("recipe_id")]\n        ingredients_list = row[header.index("ingredients_list")]\n        cuisine = row[header.index("cuisine")]\n        \n        # Add values to new row in output order\n        new_row.append(recipe_id)\n        new_row.append(cuisine)\n        \n        # Transform ingredients list from list of strings to pipe-separated string\n        if isinstance(ingredients_list, list):\n            ingredients_str = "| ".join(ingredients_list)\n        else:\n            ingredients_str = str(ingredients_list)\n        \n        new_row.append(ingredients_str)\n        \n        output_table.append(new_row)\n    \n    return output_table\n```'}]
root INFO - 07/04/2025 11:08:47 AM : Published the following metrics to Reporter : {'from': 'pre_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 0.0}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 0.003}, 'dq': 1.0}}
kafka.conn INFO - 07/04/2025 11:08:47 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]
kafka.conn INFO - 07/04/2025 11:08:47 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.
kafka.conn INFO - 07/04/2025 11:08:47 AM : <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
root INFO - 07/04/2025 11:08:47 AM : Applied plan to file 'input\source\recipes_source.json'
root INFO - 07/04/2025 11:08:48 AM : Successfully loaded the transformed file to '\output'
root INFO - 07/04/2025 11:08:48 AM : Published the following metrics to Reporter : {'from': 'post_etl_pipeline', 'contents': {'missing': {'missing_cells_percent': 0.0}, 'outliers': {'numerical_outliers_percent': 0.0}, 'duplicates': {'duplicate_rows_percent': 0.003}, 'dq': 1.0}}
kafka.producer.kafka INFO - 07/04/2025 11:08:48 AM : Closing the Kafka producer with 4294967.0 secs timeout.
kafka.conn INFO - 07/04/2025 11:08:48 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
kafka.conn INFO - 07/04/2025 11:08:48 AM : <BrokerConnection node_id=1001 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. 
