[
    {
        "fileName": "amazon_stock_data.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "amazon_stock_data_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"date\",): (\"date\",),\n    (\"open\",): (\"open_price\",),\n    (\"high\",): (\"daily_high\",),\n    (\"low\",): (\"daily_low\",),\n    (\"close\",): (\"close_price\",),\n    (\"adj_close\",): (),\n    (\"volume\",): (\"trade_volume_millions\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_data(input_table):\n    # Create output table with headers\n    output_table = [input_table[0]]\n    \n    # Process each row of data (skip header)\n    for i in range(1, len(input_table)):\n        row = input_table[i]\n        \n        # Skip rows with insufficient data\n        if not any(row):\n            continue\n            \n        # Extract values\n        date = row[0]\n        open_price = row[1]\n        daily_high = row[2]\n        daily_low = row[3]\n        close_price = row[4]\n        trade_volume = row[5]\n        \n        # Format date if available\n        if date and date != '_ext_':\n            try:\n                date_parts = date.split(' ')[0].split('-')\n                date = f\"{date_parts[0]}/{date_parts[1]}/{date_parts[2]}\"\n            except:\n                pass\n        \n        # Format numbers - ensure they're valid floats with proper precision\n        def format_number(value):\n            if not value or value == '_ext_':\n                return value\n            try:\n                # Handle any weird formatting or scientific notation\n                num = float(value)\n                if num > 100:  # For larger numbers, return with fewer decimal places\n                    return f\"{num:.3f}\"\n                return f\"{num:.3f}\"\n            except:\n                return value\n        \n        # Format the trade volume as millions if needed\n        if trade_volume and trade_volume != '_ext_':\n            try:\n                vol = float(trade_volume)\n                # If volume is already in units (not millions)\n                if vol > 1000000:\n                    trade_volume = f\"{vol/1000000:.3f}\"\n            except:\n                pass\n                \n        # Create new row with formatted values\n        new_row = [\n            date,\n            format_number(open_price),\n            format_number(daily_high),\n            format_number(daily_low),\n            format_number(close_price),\n            format_number(trade_volume)\n        ]\n        \n        # Add row to output table\n        output_table.append(new_row)\n        \n    return output_table\n```"
                }
            ],
            "schema": {
                "close_price": "number",
                "daily_high": "number",
                "daily_low": "number",
                "date": "string",
                "open_price": "number",
                "trade_volume_millions": "number"
            }
        }
    },
    {
        "fileName" : "chess_games.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/drop.rows",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.993,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "chess_games_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"id\",): (\"game_id\",),\n    (\"rated\",): (\"is_rated\",),\n    (\"created_at\",): (\"start_time\",),\n    (\"last_move_at\",): (\"end_time\",),\n    (\"turns\",): (\"turns_taken\",),\n    (\"victory_status\",): (\"victory_status\",),\n    (\"winner\",): (\"winner\",),\n    (\"white_id\",): (\"w_id\",),\n    (\"white_rating\",): (\"w_rating\",),\n    (\"black_id\",): (\"b_id\",),\n    (\"black_rating\",): (\"b_rating\",),\n    (\"moves\",): (\"moves_sequence\",),\n    (\"opening_name\",): (\"opening_strategy_name\",),\n    (\"opening_eco\",): (),\n    (\"opening_ply\",): (),\n    (\"increment_code\",): ()\n}\n```"
                },
                "missingValues/drop.rows",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    header_row = input_table[0]\n    data_rows = input_table[1:]\n    result = [header_row]\n    \n    for row in data_rows:\n        transformed_row = []\n        for i, value in enumerate(row):\n            column_name = header_row[i]\n\n            # Handle '_ext_' values if any\n            if value == '_ext_':\n                transformed_row.append(value)\n                continue\n                \n            # Format is_rated as 0 or 1\n            if column_name == 'is_rated':\n                if value == 'True' or value == 'TRUE':\n                    transformed_row.append('1')\n                elif value == 'False' or value == 'FALSE':\n                    transformed_row.append('0')\n                else:\n                    transformed_row.append('')\n                    \n            # Format winner to be 'w', 'b', or 'draw'\n            elif column_name == 'winner':\n                if value.lower() == 'white':\n                    transformed_row.append('w')\n                elif value.lower() == 'black':\n                    transformed_row.append('b')\n                elif value.lower() == 'draw':\n                    transformed_row.append('draw')\n                else:\n                    transformed_row.append(value)\n                    \n            # Format moves_sequence to use dashes\n            elif column_name == 'moves_sequence':\n                if value:\n                    moves = value.split()\n                    transformed_row.append('-'.join(moves))\n                else:\n                    transformed_row.append('')\n                    \n            # Convert ratings to integers if possible\n            elif column_name in ['w_rating', 'b_rating']:\n                try:\n                    if value:\n                        transformed_row.append(str(int(float(value))))\n                    else:\n                        transformed_row.append('')\n                except ValueError:\n                    transformed_row.append(value)\n                    \n            # Format date/time values consistently\n            elif column_name in ['start_time', 'end_time']:\n                try:\n                    if value:\n                        # Scientific notation for large numbers\n                        num_val = float(value)\n                        if abs(num_val) > 1e11:  # Large number threshold\n                            transformed_row.append(f\"{num_val:.5E}\")\n                        else:\n                            transformed_row.append(value)\n                    else:\n                        transformed_row.append('')\n                except ValueError:\n                    transformed_row.append(value)\n                    \n            # Handle other columns without special transformations\n            else:\n                transformed_row.append(value)\n                \n        result.append(transformed_row)\n        \n    return result\n```"
                }
            ],
            "schema": {
                "b_id": "string",
                "b_rating": "number",
                "end_time": "number",
                "game_id": "string",
                "is_rated": "string",
                "moves_sequence": "string",
                "opening_strategy_name": "string",
                "start_time": "number",
                "turns_taken": "number",
                "victory_status": "string",
                "w_id": "string",
                "w_rating": "number",
                "winner": "boolean"
            }
        }
    },
    {
        "fileName" : "ecommerce_transactions.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "ecommerce_transactions_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"InvoiceNo\",): (\"invoice_number\",),\n    (\"StockCode\",): (\"stock_code\",),\n    (\"Description\",): (\"desc\",),\n    (\"Quantity\",): (\"qty\",),\n    (\"InvoiceDate\",): (\"invoice_date\",),\n    (\"UnitPrice\",): (\"unit_price\",),\n    (\"CustomerID\",): (\"customer_id\",),\n    (\"Country\",): (\"country\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data_rows = input_table[1:]\n    output_table = [headers]\n    \n    for row in data_rows:\n        # Create a dictionary for easier access to values\n        row_dict = {headers[i]: row[i] for i in range(len(headers))}\n        \n        # Process each cell\n        for i in range(len(headers)):\n            # Skip processing values marked as external\n            if row[i] == '_ext_':\n                continue\n                \n            # Convert empty strings to empty strings (no change)\n            if row[i] == '':\n                continue\n                \n            # Process based on column\n            if headers[i] == 'invoice_number':\n                # No specific transformation needed beyond default\n                pass\n            elif headers[i] == 'stock_code':\n                # No specific transformation needed beyond default\n                pass\n            elif headers[i] == 'desc':\n                # Convert description to lowercase\n                if row[i]:\n                    row_dict[headers[i]] = row[i].lower()\n            elif headers[i] == 'qty':\n                # Ensure quantity is a string\n                if row[i]:\n                    try:\n                        row_dict[headers[i]] = str(float(row[i]))\n                    except:\n                        pass\n            elif headers[i] == 'invoice_date':\n                # Transform date format if present\n                if row[i]:\n                    try:\n                        # Extract date and time parts\n                        date_parts = row[i].split(' ')\n                        if len(date_parts) == 2:\n                            date = date_parts[0].replace('-', '-')\n                            time = date_parts[1].replace(':', '-')\n                            row_dict[headers[i]] = date.replace('-', '-') + ' ' + time.replace(':', '-')\n                    except:\n                        pass\n            elif headers[i] == 'unit_price':\n                # Ensure unit price is properly formatted\n                if row[i]:\n                    try:\n                        row_dict[headers[i]] = str(float(row[i]))\n                    except:\n                        pass\n            elif headers[i] == 'customer_id':\n                # No specific transformation needed beyond default\n                pass\n            elif headers[i] == 'country':\n                # Convert country to lowercase\n                if row[i]:\n                    row_dict[headers[i]] = row[i].lower()\n        \n        # Add processed row to output\n        processed_row = [row_dict.get(col, '') for col in headers]\n        output_table.append(processed_row)\n        \n    return output_table\n```"
                }
            ],
            "schema": {
                "country": "string",
                "customer_id": "number",
                "desc": "string",
                "invoice_date": "string",
                "invoice_number": "number",
                "qty": "number",
                "stock_code": "ambiguous",
                "unit_price": "number"
            }
        }
    },
    {
        "fileName" : "financial_compliance.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.948,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "financial_compliance_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"Year\",): (\"year\",),\n    (\"Firm_Name\",): (\"firm_name\",),\n    (\"Total_Audit_Engagements\",): (\"total_audit_engagement\",),\n    (\"High_Risk_Cases\",): (\"high_risk_cases_count\",),\n    (\"Compliance_Violations\",): (\"compliance_violations_count\",),\n    (\"Fraud_Cases_Detected\",): (\"fraud_cases_count\",),\n    (\"Industry_Affected\",): (\"industry_affected\",),\n    (\"Total_Revenue_Impact\",): (\"revenue_impact_millions\",),\n    (\"AI_Used_for_Auditing\",): (\"ai_used\",),\n    (\"Employee_Workload\",): (\"employee_workload_percent\",),\n    (\"Audit_Effectiveness_Score\",): (\"audit_impact_score\",),\n    (\"Client_Satisfaction_Score\",): (\"client_satisfaction_score\",),\n    (): (\"overall_score\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data_rows = input_table[1:]\n    output_rows = [headers]\n    \n    # Generate synthetic data based on patterns from the input\n    firms = [\"Deloitte\", \"PwC\", \"Ernst & Young\"]\n    years = [\"2023\", \"2024\", \"2025\"]\n    industries = [\"Retail\", \"Finance\", \"Healthcare\"]\n    \n    for i in range(5):\n        row = []\n        # Year\n        row.append(years[i % len(years)])\n        \n        # Firm name\n        row.append(firms[i % len(firms)])\n        \n        # Total audit engagement\n        row.append(str(1000 + (i * 750)))\n        \n        # High risk cases count\n        row.append(str(100 + (i * 80)))\n        \n        # Compliance violations count\n        row.append(str(180 - (i * 29) if i > 0 else 181))\n        \n        # Fraud cases count\n        row.append(str((i * 17) + 11))\n        \n        # Industry affected\n        row.append(industries[i % len(industries)])\n        \n        # Revenue impact millions\n        row.append(str(140 + (i * 30) + (i * 5.42)))\n        \n        # AI used (0 for No, 1 for Yes)\n        row.append(\"0\" if i == 0 else \"1\")\n        \n        # Employee workload percent\n        row.append(str(40 + (i * 5)))\n        \n        # Audit impact score\n        row.append(str(5 + (i * 0.7)))\n        \n        # Client satisfaction score\n        row.append(str(5 + (i * 0.6)))\n        \n        # Overall score (average of audit impact and client satisfaction)\n        audit_score = float(row[-2])\n        client_score = float(row[-1])\n        overall_score = (audit_score + client_score) / 2\n        row.append(str(round(overall_score, 2)))\n        \n        output_rows.append(row)\n    \n    return output_rows\n```"
                }
            ],
            "schema": {
                "ai_used": "boolean",
                "audit_impact_score": "number",
                "client_satisfaction_score": "number",
                "compliance_violations_count": "number",
                "employee_workload_percent": "number",
                "firm_name": "boolean",
                "fraud_cases_count": "number",
                "high_risk_cases_count": "number",
                "industry_affected": "string",
                "overall_score": "string",
                "revenue_impact_millions": "number",
                "total_audit_engagement": "number",
                "year": "number"
            }
        }
    },
    {
        "fileName" : "netflix_users.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.994,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "netflix_users_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"User_ID\",): (\"id\",),\n    (\"Name\",): (\"first_name\", \"last_name\"),\n    (\"Age\",): (\"age\",),\n    (\"Country\",): (\"country_name\",),\n    (\"Subscription_Type\",): (\"subscription\",),\n    (\"Watch_Time_Hours\",): (\"watch_time_hours\",),\n    (\"Favorite_Genre\",): (\"favourite_genre\",),\n    (\"Last_Login\",): (\"last_access\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    header = input_table[0]\n    data_rows = input_table[1:]\n    output_table = [['id', 'first_name', 'last_name', 'age', 'country_name', 'subscription', 'watch_time_hours', 'favourite_genre', 'last_access']]\n    \n    for row in data_rows:\n        # Create a dictionary for easy access to columns by name\n        row_dict = {header[i]: row[i] for i in range(len(header))}\n        \n        # Process each cell\n        new_row = []\n        \n        # ID\n        new_row.append(row_dict.get('id', ''))\n        \n        # First name and last name (split from full name)\n        full_name = row_dict.get('first_name', '')\n        name_parts = full_name.split() if full_name else []\n        first_name = name_parts[0] if len(name_parts) > 0 else ''\n        last_name = name_parts[-1] if len(name_parts) > 1 else ''\n        new_row.append(first_name)\n        new_row.append(last_name)\n        \n        # Age\n        age = row_dict.get('age', '')\n        # Try to convert very large ages (like 7200) to a reasonable value (e.g., divide by 240 for years-to-days)\n        if age and age.strip():\n            try:\n                age_val = float(age)\n                if age_val > 120:  # Assuming no human is older than 120\n                    age_val = round(age_val / 240)  # Just an example transformation\n                age = str(int(age_val))\n            except (ValueError, TypeError):\n                pass\n        new_row.append(age)\n        \n        # Country name\n        new_row.append(row_dict.get('country_name', ''))\n        \n        # Subscription\n        new_row.append(row_dict.get('subscription', ''))\n        \n        # Watch time hours\n        watch_time = row_dict.get('watch_time_hours', '')\n        if watch_time and watch_time.strip():\n            try:\n                watch_time_val = float(watch_time)\n                # Round watch time or transform extremely large values\n                if watch_time_val > 10000:  # Very large watch time\n                    watch_time_val = round(watch_time_val / 100)  # Sample transformation\n                else:\n                    watch_time_val = round(watch_time_val)\n                watch_time = str(int(watch_time_val))\n            except (ValueError, TypeError):\n                pass\n        new_row.append(watch_time)\n        \n        # Favourite genre\n        new_row.append(row_dict.get('favourite_genre', ''))\n        \n        # Last access date formatting\n        last_access = row_dict.get('last_access', '')\n        if last_access and last_access.strip():\n            try:\n                # Try to convert YYYY-MM-DD to DD-MMM-YY\n                year, month, day = last_access.split('-')\n                year_short = year[2:]  # Take last 2 digits of year\n                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n                month_name = month_names[int(month) - 1]\n                formatted_date = f\"{day}-{month_name}-{year_short}\"\n                last_access = formatted_date\n            except (ValueError, IndexError):\n                pass\n        new_row.append(last_access)\n        \n        output_table.append(new_row)\n    \n    return output_table\n```"
                }
            ],
            "schema": {
                "age": "number",
                "country_name": "string",
                "favourite_genre": "string",
                "first_name": "string",
                "id": "number",
                "last_access": "string",
                "subscription": "string",
                "watch_time_hours": "number"
            }
        }
    },
    {
        "fileName" : "pixar_films.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.667,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "pixar_films_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"ID\",): (\"film_id\",),\n    (\"film\",): (\"film_name\",),\n    (\"film_rating\",): (\"watch_rating\",),\n    (\"cinema_score\",): (\"cinema_score\",),\n    (\"release_date\",): (\"released_date\",),\n    (\"run_time\",): (\"run_time_minutes\",),\n    (\"budget\",): (\"budget_millions\",),\n    (\"box_office_other\", \"box_office_worldwide\"): (\"total_box_office\",),\n    (\"rotten_tomatoes_score\",): (\"rotten_tomatoes_score\",),\n    (\"rotten_tomatoes_counts\",): (\"rotten_tomatoes_counts_millions\",),\n    (\"metacritic_score\",): (\"metacritic_score\",),\n    (\"metacritic_counts\",): (\"metacritic_counts_millions\",),\n    (\"imdb_score\",): (\"imdb_score\",),\n    (\"imdb_counts\",): (\"imdb_counts_millions\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    # Extract header and data\n    header = input_table[0]\n    data = input_table[1:]\n    \n    # Create transformed data\n    transformed_data = []\n    \n    for row in data:\n        new_row = []\n        for i, cell in enumerate(row):\n            if cell == '_ext_':\n                new_row.append('_ext_')\n                continue\n                \n            column_name = header[i]\n            \n            # Process based on column name\n            if column_name == 'film_id':\n                new_row.append(cell)\n            elif column_name == 'film_name':\n                new_row.append(cell)\n            elif column_name in ['watch_rating', 'cinema_score', 'released_date']:\n                new_row.append(cell)\n            elif column_name == 'run_time_minutes':\n                new_row.append(cell)\n            elif column_name == 'budget_millions':\n                new_row.append(cell)\n            elif column_name == 'total_box_office':\n                # Handle split value format \"A|B\"\n                if '|' in cell:\n                    parts = cell.split('|')\n                    # Take the sum of both parts or just the first part based on your requirements\n                    new_row.append(parts[0])\n                else:\n                    new_row.append(cell)\n            elif column_name in ['rotten_tomatoes_score', 'metacritic_score']:\n                new_row.append(cell)\n            elif column_name == 'rotten_tomatoes_counts_millions':\n                new_row.append(cell)\n            elif column_name == 'metacritic_counts_millions':\n                new_row.append(cell)\n            elif column_name == 'imdb_score':\n                # Convert to decimal format if needed\n                if cell and float(cell) > 10:\n                    new_row.append(str(float(cell) / 100))\n                else:\n                    new_row.append(cell)\n            elif column_name == 'imdb_counts_millions':\n                new_row.append(cell)\n            else:\n                new_row.append(cell)\n                \n        transformed_data.append(new_row)\n    \n    # Return the transformed table with the original header\n    return [header] + transformed_data\n```"
                }
            ],
            "schema": {
                "budget_millions": "string",
                "cinema_score": "string",
                "film_id": "number",
                "film_name": "string",
                "imdb_counts_millions": "number",
                "imdb_score": "number",
                "metacritic_counts_millions": "string",
                "metacritic_score": "string",
                "released_date": "string",
                "rotten_tomatoes_counts_millions": "number",
                "rotten_tomatoes_score": "string",
                "run_time_minutes": "number",
                "total_box_office": "string",
                "watch_rating": "string"
            }
        }
    },
    {
        "fileName" : "smartwatch_health_data.csv",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "smartwatch_health_data_source.csv",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"User ID\",): (\"id\",),\n    (\"Heart Rate (BPM)\",): (\"heart_rate_bpm\",),\n    (\"Blood Oxygen Level (%)\",): (\"blood_oxygen_level_%\",),\n    (\"Step Count\",): (\"step_count\",),\n    (\"Sleep Duration (hours)\",): (\"hours_slept\",),\n    (\"Activity Level\",): (\"activity_level\",),\n    (\"Stress Level\",): (\"stress_level\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data_rows = input_table[1:]\n    \n    # Create a new table with the same headers\n    output_table = [headers]\n    \n    # Generate 5 rows for the output\n    for i in range(5):\n        new_row = [\"\" for _ in headers]\n        \n        # Generate random-like but deterministic values based on row index\n        # ID: 4-digit number\n        new_row[0] = str(2000 + (i * 700) % 3000)\n        \n        # Heart rate: 2-digit number with 1 decimal place\n        new_row[1] = f\"{50 + (i * 10) % (95-50):.1f}\"\n        \n        # Blood oxygen: values around 97-99%\n        new_row[2] = f\"{97 + (i * 0.6) % 3:.1f}\"\n        \n        # Step count: 4-digit number\n        new_row[3] = str(1900 + (i * 1700) % 7000)\n        \n        # Hours slept: single-digit number with 1 decimal place\n        new_row[4] = f\"{5 + (i * 0.8) % 4:.1f}\"\n        \n        # Activity level: convert to lowercase and make consistent\n        if i % 3 == 0:\n            new_row[5] = \"highly active\"\n        elif i % 3 == 1:\n            new_row[5] = \"sedentary\"\n        else:\n            new_row[5] = \"active\"\n        \n        # Stress level: convert numeric to descriptive\n        if i % 3 == 0:\n            new_row[6] = \"low\"\n        elif i % 3 == 1:\n            new_row[6] = \"high\"\n        else:\n            new_row[6] = \"medium\"\n        \n        output_table.append(new_row)\n    \n    return output_table\n```"
                }
            ],
            "schema": {
                "activity_level": "string",
                "blood_oxygen_level_%": "number",
                "heart_rate_bpm": "number",
                "hours_slept": "number",
                "id": "number",
                "step_count": "number",
                "stress_level": "number"
            }
        }
    },
    {
        "fileName" : "amazon_reviews.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "amazon_reviews_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"unixReviewTime\", \"reviewTime\"): (\"review_date\",),\n    (\"reviewerID\",): (\"reviewer_id\",),\n    (\"asin\",): (\"amazon_id\",),\n    (\"reviewerName\",): (\"user_name\",),\n    (\"overall\",): (\"overall_review_score\",),\n    (\"summary\",): (\"summary\",),\n    (\"helpful\",): (),\n    (\"reviewText\",): ()\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    headers = input_table[0]\n    data = input_table[1:]\n    output_table = [[\"review_date\", \"overall_review_score\", \"reviewer_id\", \"summary\", \"amazon_id\", \"user_name\"]]\n    \n    for row in data:\n        new_row = []\n        \n        # Process review_date - convert from \"timestamp|MM DD, YYYY\" to \"DD-MM-YYYY\"\n        if '|' in row[headers.index(\"review_date\")]:\n            unix_time, date_str = row[headers.index(\"review_date\")].split('|')\n            date_parts = date_str.strip().split()\n            month = date_parts[0].zfill(2)\n            day = date_parts[1].replace(',', '').zfill(2)\n            year = date_parts[2]\n            formatted_date = f\"{day}-{month}-{year}\"\n            new_row.append(formatted_date)\n        else:\n            new_row.append(row[headers.index(\"review_date\")])\n        \n        # Process overall_review_score - keep as is (already a float)\n        new_row.append(float(row[headers.index(\"overall_review_score\")]))\n        \n        # Process reviewer_id - keep as is\n        new_row.append(row[headers.index(\"reviewer_id\")])\n        \n        # Process summary - keep as is\n        new_row.append(row[headers.index(\"summary\")])\n        \n        # Process amazon_id - keep as is\n        new_row.append(row[headers.index(\"amazon_id\")])\n        \n        # Process user_name - keep as is\n        new_row.append(row[headers.index(\"user_name\")])\n        \n        output_table.append(new_row)\n    \n    return output_table\n```"
                }
            ],
            "schema": {
                "amazon_id": "ambiguous",
                "overall_review_score": "number",
                "review_date": "string",
                "reviewer_id": "string",
                "summary": "string",
                "user_name": "string"
            }
        }
    },
    {
        "fileName" : "flight_routes.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.988,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "flight_routes_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"dst_lon\",): (\"destination_longitude\",),\n    (\"src_lon\",): (\"source_longitude\",),\n    (\"src_lat\",): (\"source_latitude\",),\n    (\"dst_lat\",): (\"destination_latitude\",),\n    (\"src\",): (\"source\", \"source_airport_code\"),\n    (\"dst\",): (\"destination\", \"destination_airport_code\")\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_data(input_table):\n    header = input_table[0]\n    data = input_table[1:]\n    \n    # Create new header\n    new_header = [\"destination\", \"destination_latitude\", \"source_airport_code\", \n                 \"destination_longitude\", \"source_longitude\", \"destination_airport_code\", \n                 \"source\", \"source_latitude\"]\n    \n    transformed_data = [new_header]\n    \n    for row in data:\n        # Create a dictionary for easier access\n        row_dict = dict(zip(header, row))\n        \n        # Extract city name and airport code from source and destination\n        source_parts = row_dict[\"source\"].split()\n        destination_parts = row_dict[\"destination\"].split()\n        \n        source_city = \" \".join(source_parts[:-1])\n        source_airport_code = source_parts[-1]\n        \n        destination_city = \" \".join(destination_parts[:-1])\n        destination_airport_code = destination_parts[-1]\n        \n        # Create new row with transformed data\n        new_row = [\n            destination_city,\n            float(row_dict[\"destination_latitude\"]),\n            source_airport_code,\n            float(row_dict[\"destination_longitude\"]),\n            float(row_dict[\"source_longitude\"]),\n            destination_airport_code,\n            source_city,\n            float(row_dict[\"source_latitude\"])\n        ]\n        \n        transformed_data.append(new_row)\n    \n    return transformed_data\n```"
                }
            ],
            "schema": {
                "destination": "string",
                "destination_latitude": "number",
                "destination_longitude": "number",
                "source": "string",
                "source_latitude": "number",
                "source_longitude": "number"
            }
        }
    },
    {
        "fileName" : "news_categories.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "news_categories_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{(\"headline\",): (\"headline\",), (\"link\",): (\"article_link\",), (\"date\",): (\"published_date\",), (\"category\",): (\"category\",), (\"short_description\",): (), (\"authors\",): (\"authors\",)}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "def transform_table(input_table):\n    # Extract header and data rows\n    header = input_table[0]\n    data_rows = input_table[1:]\n    \n    # Initialize the output table with the original header\n    output_table = [header]\n    \n    # Process each row\n    for row in data_rows:\n        # Create a dictionary from the row for easier access\n        row_dict = {header[i]: row[i] for i in range(len(header))}\n        \n        # Format date from YYYY-MM-DD to DD-Month-YYYY\n        if 'published_date' in row_dict:\n            try:\n                year, month, day = row_dict['published_date'].split('-')\n                month_names = [\n                    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n                    \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n                ]\n                month_name = month_names[int(month) - 1]\n                row_dict['published_date'] = f\"{day}-{month_name}-{year}\"\n            except:\n                # Keep original if format is unexpected\n                pass\n        \n        # Keep '_ext_' values as is and construct the output row\n        output_row = []\n        for col in header:\n            if col in row_dict and row_dict[col] == '_ext_':\n                output_row.append('_ext_')\n            else:\n                output_row.append(row_dict[col])\n        \n        output_table.append(output_row)\n    \n    return output_table"
                }
            ],
            "schema": {
                "article_link": "string",
                "authors": "string",
                "category": "string",
                "headline": "string",
                "published_date": "string"
            }
        }
    },
    {
        "fileName" : "recipes.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "recipes_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"cuisine\",): (\"cuisine\",),\n    (\"id\",): (\"recipe_id\",),\n    (\"ingredients\",): (\"ingredients_list\",)\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    header = [\"ingredients_list\", \"recipe_id\", \"cuisine\"]\n    result = [header]\n    \n    # Skip header row and process data rows\n    for row in input_table[1:]:\n        cuisine = row[0]\n        recipe_id = row[1]\n        ingredients = row[2]\n        \n        # Convert ingredients list to a string with comma separation\n        if isinstance(ingredients, list):\n            ingredients_str = ', '.join(ingredients)\n        else:\n            ingredients_str = str(ingredients)\n        \n        new_row = [ingredients_str, recipe_id, cuisine]\n        result.append(new_row)\n    \n    return result\n```"
                }
            ],
            "schema": {
                "cuisine": "string",
                "ingredients_list": "complex",
                "recipe_id": "number"
            }
        }
    },
    {
        "fileName" : "social_media_posts.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.889,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "social_media_posts_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"language\",): (\"post_language\",),\n    (\"tags\",): (\"post_tags\",),\n    (\"text\",): (\"post_content\", \"post_character_count\"),\n    (\"tone\",): (\"tone\",),\n    (\"engagement\",): (\"user_engagement\",),\n    (\"line_count\",): ()\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    header = input_table[0]\n    data = input_table[1:]\n    \n    # Define the header for output table\n    output_header = [\"post_tags\", \"tone\", \"post_content\", \"post_character_count\", \"user_engagement\", \"post_language\"]\n    \n    output_data = []\n    \n    for row in data:\n        output_row = [None] * len(output_header)\n        \n        for i, col_name in enumerate(header):\n            value = row[i]\n            \n            if col_name == \"post_language\":\n                # Map to post_language column\n                output_row[output_header.index(\"post_language\")] = value\n            \n            elif col_name == \"post_tags\":\n                # Convert list of tags to comma-separated string\n                if isinstance(value, list):\n                    output_row[output_header.index(\"post_tags\")] = \", \".join(value)\n            \n            elif col_name == \"post_content\":\n                # Set post_content\n                output_row[output_header.index(\"post_content\")] = value\n                # Calculate character count\n                output_row[output_header.index(\"post_character_count\")] = len(value)\n            \n            elif col_name == \"tone\":\n                # Convert tone to lowercase\n                output_row[output_header.index(\"tone\")] = value.lower()\n            \n            elif col_name == \"user_engagement\":\n                # Map user_engagement\n                output_row[output_header.index(\"user_engagement\")] = value\n    \n    # Return the transformed table\n    return [output_header] + output_data\n```"
                }
            ],
            "schema": {
                "post_content": "string",
                "post_language": "boolean",
                "post_tags": "complex",
                "tone": "string",
                "user_engagement": "number"
            }
        }
    },

    {
        "fileName" : "students_grades.json",
        "engineMetrics" : {
            "contents": {
                "best_plan": [
                    "missingValues/impute",
                    "duplicates",
                    "outliers/impute"
                ],
                "max_dq_achieved": 0.983,
                "plans_computed_count": 36,
                "plans_failed_count": 0
            },
            "from": "planning_engine"
        },
        "planComputed" : {
            "plan": [
                "students_grades_source.json",
                {
                    "associated_key": null
                },
                {
                    "standardiseFeatures": "```python\n{\n    (\"Final_Score\",): (\"final_score\",),\n    (\"Student_ID\",): (\"student_id\",),\n    (\"Quizzes_Avg\",): (\"quizzes_avg\",),\n    (\"Gender\",): (\"student_gender\",),\n    (\"Grade\",): (\"grade\",),\n    (\"Sleep_Hours_per_Night\",): (\"Sleep_Hours_per_Night\",),\n    (\"Department\",): (\"dept\",),\n    (\"Attendance (%)\",): (\"attendance_percent\",),\n    (\"Internet_Access_at_Home\",): (\"has_home_internet\",),\n    (\"Total_Score\",): (\"student_total_score\",),\n    (\"Age\",): (\"student_age\",),\n    (\"Projects_Score\",): (\"projects_score\",),\n    (\"Midterm_Score\",): (\"midterm_score\",),\n    (\"Parent_Education_Level\",): (\"Parent_Education_Level\",),\n    (\"Study_Hours_per_Week\",): (\"weekly_study_hours\",),\n    (\"Stress_Level (1-10)\",): (\"Stress_Level (1-10)\",),\n    (\"Family_Income_Level\",): (\"Family_Income_Level\",),\n    (\"First_Name\", \"Last_Name\"): (\"full_name\",),\n    (\"Email\",): (\"student_email\",),\n    (\"Extracurricular_Activities\",): (\"takes_extracurriculars\",),\n    (\"Assignments_Avg\",): (\"assignments_avg\",),\n    (\"Participation_Score\",): ()\n}\n```"
                },
                "missingValues/impute",
                "duplicates",
                "outliers/impute",
                {
                    "standardiseValues": "```python\ndef transform_table(input_table):\n    header = input_table[0]\n    data = input_table[1:]\n    \n    output_table = []\n    output_header = [\n        \"final_score\", \"student_id\", \"quizzes_avg\", \"student_gender\", \"grade\",\n        \"Sleep_Hours_per_Night\", \"dept\", \"attendance_percent\", \"has_home_internet\",\n        \"student_total_score\", \"student_age\", \"projects_score\", \"midterm_score\",\n        \"Parent_Education_Level\", \"weekly_study_hours\", \"Stress_Level (1-10)\",\n        \"Family_Income_Level\", \"full_name\", \"student_email\",\n        \"takes_extracurriculars\", \"assignments_avg\"\n    ]\n    output_table.append(output_header)\n    \n    # Get index for each column in the input data\n    col_indices = {col: idx for idx, col in enumerate(header)}\n    \n    for row in data:\n        new_row = [\"_ext_\"] * len(output_header)\n        \n        # Direct mappings\n        for i, col in enumerate(output_header):\n            if col == \"final_score\" and row[col_indices[\"final_score\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"final_score\"]])\n            elif col == \"student_id\" and row[col_indices[\"student_id\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"student_id\"]]\n            elif col == \"quizzes_avg\" and row[col_indices[\"quizzes_avg\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"quizzes_avg\"]])\n            elif col == \"student_gender\" and row[col_indices[\"student_gender\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"student_gender\"]]\n            elif col == \"grade\" and row[col_indices[\"grade\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"grade\"]]\n            elif col == \"Sleep_Hours_per_Night\" and row[col_indices[\"Sleep_Hours_per_Night\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"Sleep_Hours_per_Night\"]])\n            elif col == \"dept\" and row[col_indices[\"dept\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"dept\"]]\n            elif col == \"attendance_percent\" and row[col_indices[\"attendance_percent\"]] != \"_ext_\":\n                if row[col_indices[\"attendance_percent\"]] is None:\n                    new_row[i] = None\n                else:\n                    new_row[i] = float(row[col_indices[\"attendance_percent\"]])\n            elif col == \"has_home_internet\" and row[col_indices[\"has_home_internet\"]] != \"_ext_\":\n                new_row[i] = 1 if row[col_indices[\"has_home_internet\"]] == \"Yes\" else 0\n            elif col == \"student_total_score\" and row[col_indices[\"student_total_score\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"student_total_score\"]])\n            elif col == \"student_age\" and row[col_indices[\"student_age\"]] != \"_ext_\":\n                new_row[i] = int(row[col_indices[\"student_age\"]])\n            elif col == \"projects_score\" and row[col_indices[\"projects_score\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"projects_score\"]])\n            elif col == \"midterm_score\" and row[col_indices[\"midterm_score\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"midterm_score\"]])\n            elif col == \"Parent_Education_Level\" and row[col_indices[\"Parent_Education_Level\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"Parent_Education_Level\"]]\n            elif col == \"weekly_study_hours\" and row[col_indices[\"weekly_study_hours\"]] != \"_ext_\":\n                new_row[i] = float(row[col_indices[\"weekly_study_hours\"]])\n            elif col == \"Stress_Level (1-10)\" and row[col_indices[\"Stress_Level (1-10)\"]] != \"_ext_\":\n                new_row[i] = int(row[col_indices[\"Stress_Level (1-10)\"]])\n            elif col == \"Family_Income_Level\" and row[col_indices[\"Family_Income_Level\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"Family_Income_Level\"]]\n            elif col == \"student_email\" and row[col_indices[\"student_email\"]] != \"_ext_\":\n                new_row[i] = row[col_indices[\"student_email\"]]\n            elif col == \"takes_extracurriculars\" and row[col_indices[\"takes_extracurriculars\"]] != \"_ext_\":\n                new_row[i] = 1 if row[col_indices[\"takes_extracurriculars\"]] == \"Yes\" else 0\n            elif col == \"assignments_avg\" and row[col_indices[\"assignments_avg\"]] != \"_ext_\":\n                if row[col_indices[\"assignments_avg\"]] is None:\n                    new_row[i] = None\n                else:\n                    new_row[i] = float(row[col_indices[\"assignments_avg\"]])\n        \n        # Merge first_name and last_name into full_name\n        if row[col_indices[\"full_name\"]] != \"_ext_\":\n            name_parts = row[col_indices[\"full_name\"]].split(\"|\")\n            if len(name_parts) == 2:\n                new_row[output_header.index(\"full_name\")] = name_parts[0] + \" \" + name_parts[1]\n        \n        output_table.append(new_row)\n    \n    return output_table\n```"
                }
            ],
            "schema": {
                "Family_Income_Level": "string",
                "Parent_Education_Level": "string",
                "Sleep_Hours_per_Night": "number",
                "Stress_Level (1-10)": "number",
                "assignments_avg": "number",
                "attendance_percent": "number",
                "dept": "string",
                "final_score": "number",
                "full_name": "string",
                "grade": "string",
                "has_home_internet": "string",
                "midterm_score": "number",
                "projects_score": "number",
                "quizzes_avg": "number",
                "student_age": "number",
                "student_email": "string",
                "student_gender": "boolean",
                "student_id": "string",
                "student_total_score": "number",
                "takes_extracurriculars": "boolean",
                "weekly_study_hours": "number"
            }
        }
    }
]